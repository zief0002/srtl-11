---
title: "Recursive Partitioning and Pruning: Part II"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \usepackage{float}
   - \usepackage{lscape}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Bembo Std"
sansfont: "Helvetica Neue UltraLight"
monofont: Inconsolata
urlcolor: "umn2"
always_allow_html: yes
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.heigh=6, out.width='4in', fig.align = 'center', fig.pos = 'H')
library(dplyr)
library(ggplot2)
library(ggsci)
library(gridExtra)
library(scales)
library(DiagrammeR)

bechdel_train = readr::read_csv("~/Dropbox/srtl-11/r-stuff/cart-app/data/bechdel-training-data.csv")

weather = data.frame(
  temp = c(85, 80, 83, 61, 89, 86, 64, 72, 69, 75, 75, 72, 81, 71, 60, 77, 82, 62, 70, 90, 90, 91),
  humd = c(85, 90, 86, 98, 60, 70, 65, 95, 70, 80, 70, 90, 75, 91, 100, 85, 97, 90, 95, 95, 85, 75),
  play = c("No", "No", "Yes", "Yes", "Yes", "No", "Yes", "No", "Yes", "Yes", "Yes", "No", "Yes", "No", "No", "No", "Yes", "Yes", "No", "No", "No", "No")
)

my_lab = data.frame(
  temp = c(65, 64, 88),
  humd = c(80, 98, 80),
  lab = c("1", "2", "2")
)



```


Consider the following data that show an outcome (circles OR triangles) based on values for two predictors, X1 and X2.


```{r echo=FALSE, out.width='5in'}
ggplot(data = weather, aes(x = temp, y = humd)) +
  geom_point(aes(fill = play, shape = play), size = 5, color = "black") +
  theme_bw() +
  scale_x_continuous(
    name = "X1", 
    limits = c(60, 95),  
    minor_breaks = 60:95,  
    breaks = seq(from = 60, to = 95, by = 5)
    ) +
  scale_y_continuous(
    name = "X2",    
    limits = c(60, 100), 
    minor_breaks = 60:100, 
    breaks = seq(from = 60, to = 100, by = 5)
    ) +
  scale_shape_manual(values = c(21, 24)) +
  scale_fill_simpsons() +
  guides(fill = FALSE, shape = FALSE)
```


1. Draw *a single horizontal or vertical line* through the data that results in the highest possible classification accuracy for the data. Indicate the side of the line that is to be classified as "triangles" and that which will be classified as "circles". Label this line as Decision Rule 1.

\vspace{2em}

2. Consider the cases that were classified as circles by your first decision rule. Draw *a single horizontal or vertical line* through the data that results in the highest possible classification accuracy for those cases. Indicate the side of the line that is to be classified as "triangles" and that which will be classified as "circles". Label this line as Decision Rule 2.1.

\vspace{2em}

3. Consider the cases that were classified as triangles by your first decision rule. Draw *a single horizontal or vertical line* through the data that results in the highest possible classification accuracy for those cases. Indicate the side of the line that is to be classified as "triangles" and that which will be classified as "circles". Label this line as Decision Rule 2.2.

\newpage

4. For each of the Stage 2 rules, consider the cases that were classified as circles. Can you draw *a single horizontal or vertical line* through the data that improves the classification accuracy for those cases? What about for those cases that were classified as triangles? Continue the partitioning and labeling of stages until the classification accuracy can no longer be improved. (This may happen prior to reaching 100\% classification accuracy.)

\vspace{2em}

5. Draw out the decision tree based on your partitioning.

\newpage



## Tree Trimming/Pruning

Below we provide you with 20 new validation cases to use in the model evaluation process. 

```{r echo=FALSE, results='asis', out.width='5in', fig.caption='Plot of the validation data.', fig.pos='H', message=FALSE}

tab_03 = read.csv("data/tennis-validation-data.csv") %>%
  select(X1, X2, Outcome = play) %>%
  mutate(
    Outcome = as.character(if_else(Outcome == "No", "Circle", "Triangle"))
    ) %>%
  head(10)

tab_04 = read.csv("data/tennis-validation-data.csv") %>%
  select(X1, X2, Outcome = play) %>%
  mutate(
    Outcome = as.character(if_else(Outcome == "No", "Circle", "Triangle"))
    ) %>%
  tail(10)

cbind(tab_03, tab_04) %>%
  knitr::kable(align = "c")


readr::read_csv("data/tennis-validation-data.csv") %>%
  select(X1, X2, Outcome = play) %>%
ggplot(aes(x = X1, y = X2, fill = Outcome, shape = Outcome)) +
  geom_point(size = 5, color = "black") +
  scale_shape_manual(values = c(21, 24)) +
  scale_fill_simpsons() +
  theme_bw() +
  scale_x_continuous(
    name = "X1", 
    limits = c(50, 100),  
    minor_breaks = 50:100,  
    breaks = seq(from = 50, to = 100, by = 5)
  ) +
  scale_y_continuous(
    name = "X2",    
    limits = c(50, 110), 
    minor_breaks = 50:110, 
    breaks = seq(from = 50, to = 110, by = 5)
  ) +
  guides(shape = FALSE, fill = FALSE)
```

6. Begin the process of evaluation by computing the classification accuracy for the model that classified 100\% of the training cases correctly. Note: *Do not update or change any of the decision rules. Just use them to compute the classification accuracy.*

\newpage

7. Prune the tree using a threshold for improvement of at least 3 percentage points. Continue pruning rules until you adopt a "final" tree. Draw out the "final" decision tree based on your pruning and report the classification accuracy for this model.










